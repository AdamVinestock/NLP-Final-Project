fixed_context = "This sentence was generated by a Large Language Model:"
context_policies = [None, 'previous_sentence', None, 'previous_sentence', 'summary', 'summary_and_previous_sentence',
                    'previous_3_sentences', 'previous_3_sentences']
contexts = [None, None, fixed_context, fixed_context, None, None, None, fixed_context]
policy_names = ['no_context', 'previous_sen', 'naive', 'naive_and_prev', 'summary', 'summary_and_prev', 'prev_3',
                'naive_and_prev_3']


class Pipe_line():
    def __init__(self, dataset_name, model, tokenizer, context_policies, contexts, policy_names, n=10):
        self.dataset_name = dataset_name
        self.context_policies = context_policies
        self.contexts = contexts
        self.policy_names = policy_names
        self.n = n
        self.sentence_detector = PerplexityEvaluator(model, tokenizer)
        self.human_dataset, self.machine_dataset = self.SplitDataset()
        self.parsers_list = self.CreateParsers()
        self.datasets_dict = {'human': self.human_dataset, 'machine': self.machine_dataset}

    def SplitDataset(self):
        human_dataset = None
        machine_dataset = None
        if self.dataset_name == "wiki-intro-long":
            human_dataset = get_text_from_wiki_long_dataset(shuffle=False, text_field='human_text')
            machine_dataset = get_text_from_wiki_long_dataset(shuffle=False, text_field='machine_text')
        elif self.dataset_name == "news-chatgpt-long":
            human_dataset = get_text_from_chatgpt_news_long_dataset(shuffle=False, text_field='article')
            machine_dataset = get_text_from_chatgpt_news_long_dataset(shuffle=False, text_field='chatgpt')
        elif self.dataset_name == "ChatGPT-Research-Abstracts":
            human_dataset = get_text_from_chatgpt_abstracts_dataset(shuffle=False, text_field='real_abstract')
            machine_dataset = get_text_from_chatgpt_abstracts_dataset(shuffle=False, text_field='generated_abstract')

        shortened_human_dataset = dataset.select(range(self.n))
        shortened_machine_dataset = dataset.select(range(self.n))

        return shortened_human_dataset, shortened_machine_dataset

    def CreateParsers(self):
        parsers = []
        for policy, context in zip(self.context_policies, self.contexts):
            parser = PrepareSentenceContext(policy, context)
            parsers.append(parser)

        return parsers

    def CalculatePerplexity(self):
        # Perform log ppx calculation
        human_df_list = []
        machine_df_list = []
        i = 0
        for parser in self.parsers_list:
            for dataset in self.datasets_dict:
                csv_name = self.policy_names[i] + "_" + str(dataset) + '.csv'
                iterate_over_texts(self.datasets_dict[dataset], self.sentence_detector, parser, csv_name)
                if dataset == 'human':
                    df = pd.read_csv(csv_name)
                    human_df_list.append(df)
                elif dataset == 'machine':
                    df = pd.read_csv(csv_name)
                    machine_df_list.append(df)
            i += 1

        return human_df_list, machine_df_list

    def Calculate_Perplexity_Differance(self, human_df_list, machine_df_list):
        results = {'human': {}, 'machine': {}}
        #differances_result = {policy}

        #  Select the "response" column and compute the mean

        for dataset in self.datasets_dict:
            if dataset == 'human':
                policy_df_list = human_df_list
            if dataset == 'machine':
                policy_df_list = machine_df_list

            for name, policy in enumerate(policy_df_list):
                policy_results = {'id': [], 'mean_perplexity': []}
                for id in self.datasets_dict[dataset]['id']:
                    filtered_policy_df = policy[policy['name'] == id]
                    mean = filtered_policy_df["response"].mean()

                    # Saving results
                    policy_results['id'].append[id]
                    policy_results['id'].append[mean]
                    results[dataset][self.policy_names[name]] = policy_results

        return results



