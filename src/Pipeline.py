from many_atomic_detections import process_text, iterate_over_texts
from src.PerplexityEvaluator import PerplexityEvaluator
from src.PrepareSentenceContext import PrepareSentenceContext
from src.dataset_loaders import (get_text_from_wiki_long_dataset,
                                 get_text_from_chatgpt_news_long_dataset,
                                 get_text_from_chatgpt_abstracts_dataset)
import pandas as pd

fixed_context = "This sentence was generated by a Large Language Model:"
context_policies = [None, 'previous_sentence', None, 'previous_sentence', 'summary', 'summary_and_previous_sentence',
                    'previous_3_sentences', 'previous_3_sentences']
contexts = [None, None, fixed_context, fixed_context, None, None, None, fixed_context]
policy_names = ['no_context', 'previous_sen', 'naive', 'naive_and_prev', 'summary', 'summary_and_prev', 'prev_3',
                'naive_and_prev_3']


class PipelineClass():
    def __init__(self, dataset_name, model, tokenizer, context_policies, contexts, policy_names, n=10):
        self.model = model
        self.dataset_name = dataset_name
        self.context_policies = context_policies
        self.contexts = contexts
        self.policy_names = policy_names
        self.n = n
        self.sentence_detector = PerplexityEvaluator(model, tokenizer)
        self.human_dataset, self.machine_dataset = self.SplitDataset()
        self.parsers_list = self.CreateParsers()
        self.datasets_dict = {'human': self.human_dataset, 'machine': self.machine_dataset}

    def SplitDataset(self):
        human_dataset = None
        machine_dataset = None
        if self.dataset_name == "wiki-intro-long":
            human_dataset = get_text_from_wiki_long_dataset(shuffle=False, text_field='human_text')
            machine_dataset = get_text_from_wiki_long_dataset(shuffle=False, text_field='machine_text')
        elif self.dataset_name == "news-chatgpt-long":
            human_dataset = get_text_from_chatgpt_news_long_dataset(shuffle=False, text_field='article')
            machine_dataset = get_text_from_chatgpt_news_long_dataset(shuffle=False, text_field='chatgpt')
        elif self.dataset_name == "ChatGPT-Research-Abstracts":
            human_dataset = get_text_from_chatgpt_abstracts_dataset(shuffle=False, text_field='real_abstract')
            machine_dataset = get_text_from_chatgpt_abstracts_dataset(shuffle=False, text_field='generated_abstract')

        shortened_human_dataset = human_dataset.select(range(self.n))
        shortened_machine_dataset = machine_dataset.select(range(self.n))

        return shortened_human_dataset, shortened_machine_dataset

    def CreateParsers(self):
        parsers = []
        for policy, context in zip(self.context_policies, self.contexts):
            parser = PrepareSentenceContext(context_policy = policy, context = context)
            parsers.append(parser)

        return parsers

    def CalculatePerplexity(self):
        # Perform log ppx calculation
        human_responses = []
        machine_responses = []
        i = 0
        for parser in self.parsers_list:
            for dataset in self.datasets_dict:  # human or machine
                csv_name = str(self.dataset_name) + "_" + str(dataset) + "_" + str(self.model.config.model_type) + "_" + self.policy_names[i] + '.csv'
                iterate_over_texts(self.datasets_dict[dataset], self.sentence_detector, parser, csv_name)
                if dataset == 'human':
                    df = pd.read_csv(csv_name)
                    human_responses.append(df)
                elif dataset == 'machine':
                    df = pd.read_csv(csv_name)
                    machine_responses.append(df)
            i += 1

        return human_responses, machine_responses

    def calc_ppx_diff_instance(self, human_responses, machine_responses):
        results = {'human': {}, 'machine': {}}

        #  Select the "response" column and compute the mean
        for dataset in self.datasets_dict: # human or machine
            if dataset == 'human':
                responses = human_responses
            elif dataset == 'machine':
                responses = machine_responses

            for i, policy in enumerate(responses):
                policy_results = {'id': [], 'mean_perplexity': []}
                for id in self.datasets_dict[dataset]['id']:
                    filtered_policy_df = policy[policy['name'] == id]
                    mean = filtered_policy_df["response"].mean()

                    # Saving results
                    policy_results['id'].append[id]
                    policy_results['id'].append[mean]
                    results[dataset][self.policy_names[i]] = policy_results

        return results

    def calc_ppx_diff_chunk(self, human_responses, machine_responses):

        results = {}
        for i, policy in enumerate(human_responses):
            policy_results = (human_responses[i]['response'] - machine_responses[i]['response']).mean()
            results[self.policy_names[i]] = (human_responses[i]['response'].mean(), machine_responses[i]['response'].mean(), policy_results)

        return results




